---
title: "Chapter 4"
author: "Dominik Klepl"
date: "25 2 2018"
output: html_document
---
```{r}
library(rethinking)
library(ggplot2)
```


## R code 4.1
```{r}
#simulate a normal distribution - 1000 people on field, flipping coing (heads=step right, tails=step left), everybody flips 16 times - look at distribution of people's positon on the field (most at 0 etc.) - steps are independent from each other (sometimes 1 m, sometimes 0.9 m etc)

pos = replicate( 1000 , sum( runif(16,-1,1) ) )

plot(density(pos))
```

## R code 4.2-5
```{r}
#norm distribution emerges also from multiplication of random numbers from same distr.

#growing cell, combining loci affect that by 0%-10% - draw 12 samples of loci and multiply them +1 to get the resulting size of cell
prod( 1 + runif(12,0,0.1) )

## R code 4.3
#do this 10 000 times
growth = replicate( 10000 , prod( 1 + runif(12,0,0.1) ) )
dens( growth , norm.comp=TRUE ) #it converges to gaussian, of course

## R code 4.4
#when the sample fluctuation is big it takes longer for gaussian to emerge
big = replicate( 800000 , prod( 1 + runif(12,0,0.5) ) )
small = replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )

dens(big, norm.comp=TRUE )
dens(small, norm.comp=TRUE )

## R code 4.5
#when the fluctuations are big than it takes a lot samples to create a gaussian - but logs of the products form it at low number of samples
log.big = replicate( 10000 , log(prod(1 + runif(12,0,0.5))) )

dens(log.big, norm.comp=TRUE )

```

## R code 4.7-10 Setting up the data
```{r}
#building a gaussian model of height
data(Howell1)
d = Howell1

## R code 4.8
str( d )

## R code 4.9
d$height

## R code 4.10
#for now use only adults
d2 = d[ d$age >= 18 , ]

#outcome of the model is height - adult's height is gaussian-ish - basically a sum of growths
dens(d2$height)


```

## R code 4.11-13 (p.83)
```{r}
#creating a model - first only estimating the distribution of height - surely is gaussian
  #Normal has 2 parameters - mu (mean) and sigma (sd) - both need a prior distribution

#prior for mu - broad Gaussian prior - I'm 178 too so let's assume that that's the average human height and that 95% of people have height in interval 178+-40 (2*sigma)
curve( dnorm( x , 178 , 20 ) , from=100 , to=250 )

#prior for sigma - uniform prior  - sigma is uniformly likely to be between 0 and 50
  #assuming that 95% of individual heights are within 100 cm of average height
curve( dunif( x , 0 , 50 ) , from=-10 , to=60 )

#prior for height is not explicitly specified but we can simulate heights from the priors of parameters - combinations of parameters' priors are possible distributions of height
sample_mu = rnorm( 1e4 , 178 , 20 )
sample_sigma = runif( 1e4 , 0 , 50 )
prior_h = rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_h )
```

## R code 4.14
Ge the posterior by analytical computation - get the target distribution that will be later approximated with grid and quadratic approximations. Computationally expensive and with more parameters even impossible - but just for now :-)
```{r}
mu.list = seq( from=140, to=160 , length.out=200 )
sigma.list = seq( from=4 , to=9 , length.out=200 )
post = expand.grid( mu=mu.list , sigma=sigma.list )
post$LL = sapply( 1:nrow(post) , function(i) sum( dnorm(
                d2$height ,
                mean=post$mu[i] ,
                sd=post$sigma[i] ,
                log=TRUE ) ) )
post$prod = post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
    dunif( post$sigma , 0 , 50 , TRUE )
post$prob = exp( post$prod - max(post$prod) )

## R code 4.15
contour_xyz( post$mu , post$sigma , post$prob )

## R code 4.16
image_xyz( post$mu , post$sigma , post$prob )
```
## R code 4.17
Sample from the posterior to inspect it a little
```{r}
#first sample 10 000 rows (combinations of parameters) in proportion to post$prob
sample.rows = sample( 1:nrow(post) , size=1e4 , replace=TRUE ,
    prob=post$prob )

#and now pull parameter values from these rows
sample.mu = post$mu[ sample.rows ]
sample.sigma = post$sigma[ sample.rows ]

## R code 4.18
#and plot these values against each other
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.2) )
```
## R code 4.19
Now the posterior densities of parameters can be summarized
```{r}
dens( sample.mu )
dens( sample.sigma )
#both kinda gaussian

## R code 4.20
HPDI( sample.mu )
HPDI( sample.sigma )

#these samples are just vectors so we can extract any statistical feature
median(sample_mu)
```

## R code 4.21
From overthinking box: posterior density of sd is actually not gaussian - has thick right tail (model is confident in that true variance is not much lower than the estimate but less in that the variance could be higher =>the right tail) - showing the exxagerated version of that with only 20 data points.
```{r}
d3 = sample( d2$height , size=20 )

## R code 4.22
mu.list = seq( from=150, to=170 , length.out=200 )
sigma.list = seq( from=4 , to=20 , length.out=200 )
post2 = expand.grid( mu=mu.list , sigma=sigma.list )
post2$LL = sapply( 1:nrow(post2) , function(i)
    sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] ,
    log=TRUE ) ) )
post2$prod = post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
    dunif( post2$sigma , 0 , 50 , TRUE )
post2$prob = exp( post2$prod - max(post2$prod) )
sample2.rows = sample( 1:nrow(post2) , size=1e4 , replace=TRUE ,
    prob=post2$prob )
sample2.mu = post2$mu[ sample2.rows ]
sample2.sigma = post2$sigma[ sample2.rows ]
plot( sample2.mu , sample2.sigma , cex=0.5 ,
    col=col.alpha(rangi2,0.1) ,
    xlab="mu" , ylab="sigma" , pch=16 )

## R code 4.23
dens( sample2.sigma , norm.comp=TRUE ) #apparantly not normal - be aware of that when doing quadratic approximation
```


## R code 4.24
Let's repeat the stuff above with using quadratic approximation now
```{r}
library(rethinking)
data(Howell1)
d = Howell1
d2 = d[ d$age >= 18 , ]

#now define the model - same as before
flist = alist(
    height ~ dnorm( mu , sigma ) , #hi~ Normal (mu,sigma)
    mu ~ dnorm( 178 , 20 ) , #mu ~ Normal(178,20)
    sigma ~ dunif( 0 , 50 ) #sigma ~ Uniform(0,50)
)

#and fit the model to the data
m4.1 = map( flist , data=d2 )

#look at summary of the model
precis( m4.1 )

```


## R code 4.28
Quadratic approximation starts at random combination of parameters values. But we can give the model a starting point where we think the MAP values are likely to be.
```{r}
start = list(
    mu=mean(d2$height),
    sigma=sd(d2$height)
)

m4.2 = map(flist=flist, start=start, data=d2)

#results are the same but the process is likely to be faster
precis(m4.2)
precis(m4.1)
```


## R code 4.29
Now use a more concentrated mu prior (sd=0.1)
```{r}
m4.3 = map(alist(height ~ dnorm(mu , sigma) ,
                 mu ~ dnorm(178 , 0.1) ,
                 sigma ~ dunif(0 , 50)) ,
                 data = d2)

precis(m4.3)

#mu almost didnt move from the prior - because it's strong prior
#sigma changed a lot, although its prior did not
```

## R code 4.30
Since the model has 2 parameters the quadratic approximation approximated a Gaussian two-dimensional distribution. Which is why it doesnt compute only mean and sd but also covariance among all pairs of parameters. To describe multidimensional Gaussian distribution we need list of means and matrix of variances and covariances
```{r}
#variance-covariance matrix for model 4.1
vcov( m4.1 )

#the matrix above has two elements - 
  #1. variances of parameters
  #2. correlation matrix
diag( vcov( m4.1 ) )
cov2cor( vcov( m4.1 ) )
```

## R code 4.32
Now we can sample from the posterior that was approximated. We sample 2-element vectors from multi-dimensional Gaussian distribution.
```{r}
library(rethinking)
post = extract.samples( m4.1 , n=1e4 )
head(post)

## R code 4.33
precis(post)

plot(post) #looks similar to plot from grid approximation
```


## R code 4.34
Overthinking box - this is the underlying function of extract.samples() used above. mvrnorm is multivariate version of rnorm(). It simulates randomly from multivariate gaussian.
```{r}
post = MASS::mvrnorm( n=1e4 , mu=coef(m4.1) , Sigma=vcov(m4.1) ) #it returns matrix instead of data.frame as extract.samples does
```


## R code 4.35
Overthinking again :-) Distribution of sigma is not normal and so the quadratic assumption can lead to troubles. To avoid that we can estimate logs of sigma instead, those are likely to be gaussian. It won't make much change in this example. Is useful sometimes because it's robust trick. Relates to link functions.
```{r}
m4.1_logsigma = map(
        alist(
            height ~ dnorm( mu , exp(log_sigma) ) , #log sigma needs to exponentiated here to get the normal sigma again
            mu ~ dnorm( 178 , 20 ) ,
            log_sigma ~ dnorm( 2 , 10 ) #log sigma can have a normal prior instead of uniform now
        ) , data=d2 )

## R code 4.36
post_log = extract.samples( m4.1_logsigma )
sigma = exp( post_log$log_sigma )

dens(sigma)
dens(post_log$log_sigma)
```

## R code 4.37
Let's add a predictor now. First just look at covariance of height with weight.
```{r}
plot( d2$height ~ d2$weight )
```

## R code 4.38
```{r}
# load data again, since it's a long way back
library(rethinking)
data(Howell1)
d = Howell1
d2 = d[ d$age >= 18 , ]

# fit model
m4.3 = map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + b*weight ,
        a ~ dnorm( 156 , 100 ) ,
        b ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
    ) ,
    data=d2 )

## R code 4.39 - overthinking
#exactly the same model as above, it's just more similar to the actual computation
# m4.3 = map(
#     alist(
#         height ~ dnorm( a + b*weight , sigma ) ,
#         a ~ dnorm( 178 , 100 ) ,
#         b ~ dnorm( 0 , 10 ) ,
#         sigma ~ dunif( 0 , 50 )
#     ) ,
#     data=d2 )
```


## R code 4.40
```{r}
precis( m4.3 )

## R code 4.41
precis( m4.3 , corr=TRUE )
```

## R code 4.42
A and b are strongly negatively correlated. Now it doesnt matter so much but with more complex models it might be difficult to fit them to data. One technique to reduce the correlation is "Centering". Subtract mean value from each value
```{r}
d2$weight.c = d2$weight - mean(d2$weight)

#mean of the centered weight is 0
round(mean(d2$weight.c),3)

#let's fit the model again using the centered weight
m4.4 = map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + b*weight.c ,
        a ~ dnorm( 178 , 100 ) ,
        b ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
    ) ,
    data=d2 )

## R code 4.44
precis( m4.4 , corr=TRUE ) #now no parameters are correlated
#and valued of a is now the same as mean of height in raw data
mean(d2$height)
```

## R code 4.45
Now let's plot the MAP values for mean height over the actual data.
```{r}
plot( height ~ weight , data=d2 )
abline( a=coef(m4.3)["a"] , b=coef(m4.3)["b"] )
```


## R code 4.46
There might a lot of uncertainty around the MAP values plotted above. There might be a lot of other lines that have similar probability. Or the ditribution around the MAP line is narrow and there is not much uncertainty. Let's plot that there too.
```{r}
#sample from the posterior
post = extract.samples( m4.3 )

## R code 4.47
post[1:5,]
```


## R code 4.48
It's easier to see the scatter of the lines with less data so let's refit the model using only 10 datapoints.
```{r}
N = 10
dN = d2[ 1:N , ]
mN = map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + b*weight ,
        a ~ dnorm( 178 , 100 ) ,
        b ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
    ) , data=dN )

## R code 4.49 - let's plot 20 of the lines from the model
# extract 20 samples from the posterior
post = extract.samples( mN , n=20 )

# display raw data and sample size
plot( dN$weight , dN$height ,
    xlim=range(d2$weight) , ylim=range(d2$height) ,
    col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N))

# plot the lines, with transparency
for ( i in 1:20 )
    abline( a=post$a[i] , b=post$b[i] , col=col.alpha("black",0.3) )
```


## R code 4.50
```{r}
#get 10 000 mu values for 50 kg person
mu_at_50 = post$a + post$b * 50

#and plot the density of these mus
dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )

## R code 4.52
HPDI( mu_at_50 , prob=0.89 )
```

## R code 4.53
Now repeat the above for all values of weight in the data used to fit the model and get the 89% HPDI.
```{r}
post = extract.samples(m4.3)
mu = link( m4.3 )
str(mu) #that's slightly too big and not really what we want now


# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq = seq( from=25 , to=70 , by=1 )

# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu = link( m4.3 , data=data.frame(weight=weight.seq) )
str(mu)

## R code 4.55
# use type="n" to hide raw data
plot(height ~ weight , d2 , type = "n")
for (i in 1:100)
points(weight.seq , mu[i,] , pch = 16)
```


## R code 4.56
```{r}
# summarize the distribution of mu
mu.mean = apply( mu , 2 , mean )
mu.HPDI = apply( mu , 2 , HPDI , prob=0.89 )

```

```{r}
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )

# draw MAP line
lines( weight.seq , mu.mean )

# draw HPDI region for line
shade( mu.HPDI , weight.seq )
```

## R code 4.58
```{r}
#just and explanation how link function works
post = extract.samples(m4.3)
mu.link = function(weight) post$a + post$b*weight
weight.seq = seq( from=25 , to=70 , by=1 )
mu = sapply( weight.seq , mu.link )
```


## R code 4.59
```{r}
#instead of simulating possible values of mu let's simulate height given the weight using not only mu (average) but also sigma (the uncertainty around mu)
weight.seq = seq( from=25 , to=70 , by=1 )
sim.height = sim( m4.3 , data=list(weight=weight.seq) ,n=5e3)
str(sim.height)
```

## R code 4.60
```{r}
#these simulations can be summarized as any other samples
height.PI = apply( sim.height , 2 , PI , prob=0.89 )

## R code 4.61
# plot raw data
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )

# draw MAP line
lines( weight.seq , mu.mean )

# draw HPDI region for line
shade( mu.HPDI , weight.seq )

# draw PI region for simulated heights
shade( height.PI , weight.seq )
```


## R code 4.63
Explanation of how sim function works
```{r}
post = extract.samples(m4.3)
weight.seq = 25:70
sim.height = sapply( weight.seq , function(weight)
    rnorm(
        n=nrow(post) ,
        mean=post$a + post$b*weight ,
        sd=post$sigma ) )
height.PI = apply( sim.height , 2 , PI , prob=0.89 )
```

## R code 4.64
Polynomial Regression
```{r}
library(rethinking)
data(Howell1)
d = Howell1
str(d)

#now children are added too, let's plot it
library(ggplot2)

ggplot(d,aes(x=weight, y=height))+
  geom_point()
```

```{r}
## R code 4.65
#standardize the weight- easier to fit (get's rid of large numbers), might make interpretation easier (or harder)
d$weight.s = ( d$weight - mean(d$weight) )/sd(d$weight)

## R code 4.66
#create a squared standard weight - easier to fit than doing it in the formula
d$weight.s2 = d$weight.s^2

#fit the quadratic model with weak priors
m4.5 = map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + b1*weight.s + b2*weight.s2 ,
        a ~ dnorm( 178 , 100 ) ,
        b1 ~ dnorm( 0 , 10 ) ,
        b2 ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
    ) ,
    data=d )

## R code 4.67
precis( m4.5 )
```

## R code 4.68
the beta parameters are now difficult to understand, let's plot them for transparency
```{r}
weight.seq = seq( from=-2.2 , to=2 , length.out=30 )
pred_dat = list( weight.s=weight.seq , weight.s2=weight.seq^2 )
mu = link( m4.5 , data=pred_dat )
mu.mean = apply( mu , 2 , mean )
mu.PI = apply( mu , 2 , PI , prob=0.89 )
sim.height = sim( m4.5 , data=pred_dat )
height.PI = apply( sim.height , 2 , PI , prob=0.89 )

## R code 4.69
plot( height ~ weight.s , d , col=col.alpha(rangi2,0.5) )
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )
```

## R code 4.70
d$weight.s3 = d$weight.s^3
m4.6 = map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + b1*weight.s + b2*weight.s2 + b3*weight.s3 ,
        a ~ dnorm( 178 , 100 ) ,
        b1 ~ dnorm( 0 , 10 ) ,
        b2 ~ dnorm( 0 , 10 ) ,
        b3 ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
    ) ,
    data=d )

## R code 4.71
plot( height ~ weight.s , d , col=col.alpha(rangi2,0.5) , xaxt="n" )

## R code 4.72
at = c(-2,-1,0,1,2)
labels = at*sd(d$weight) + mean(d$weight)
axis( side=1 , at=at , labels=round(labels,1) )

## R code 4.73
plot( height ~ weight , data=Howell1 ,
    col=col.alpha(rangi2,0.4) )
```

