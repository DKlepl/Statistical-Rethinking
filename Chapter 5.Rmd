---
title: "Chapter 5"
author: "Dominik Klepl"
date: "4 3 2018"
output: html_document
---

## R code 5.1
Fit model Divorce ~ MedianAgeMarriage
```{r}
# load data
library(rethinking)
data(WaffleDivorce)
d = WaffleDivorce

# standardize predictor - it's good practice anytime
d$MedianAgeMarriage.s = (d$MedianAgeMarriage-mean(d$MedianAgeMarriage))/
    sd(d$MedianAgeMarriage)

# fit model
m5.1 = map(
  alist(
    Divorce ~ dnorm(mu , sigma) ,
    #normal distribution of outcome
    mu <- a + bA * MedianAgeMarriage.s ,
    #intercept +ß*predictor
    a ~ dnorm(10 , 10) ,
    bA ~ dnorm(0 , 1) ,
    sigma ~ dunif(0 , 10)
  ) ,
  data = d
)

precis(m5.1)
```

## R code 5.2
```{r}
# compute percentile interval of mean
MAM.seq <- seq( from=-3 , to=3.5 , length.out=30 )
mu <- link( m5.1 , data=data.frame(MedianAgeMarriage.s=MAM.seq) )
mu.PI <- apply( mu , 2 , PI )

# plot it all
plot( Divorce ~ MedianAgeMarriage.s , data=d , col=rangi2 )
abline( m5.1 )
shade( mu.PI , MAM.seq )
```

## R code 5.3
Fit a regression divorce~marriage
```{r}
d$Marriage.s <- (d$Marriage - mean(d$Marriage))/sd(d$Marriage)
m5.2 <- map(
  alist(
    Divorce ~ dnorm(mu , sigma) ,
    mu <- a + bR * Marriage.s ,
    a ~ dnorm(10 , 10) ,
    bR ~ dnorm(0 , 1) ,
    sigma ~ dunif(0 , 10)
  ) ,
  data = d
)

precis(m5.2)

# compute percentile interval of mean
M.seq <- seq( from=-3 , to=3.5 , length.out=30 )
mu_2 <- link( m5.2 , data=data.frame(Marriage.s=M.seq) )
mu.PI_2 <- apply( mu_2 , 2 , PI )

# plot it all
plot( Divorce ~ Marriage.s , data=d , col=rangi2 )
abline( m5.2 )
shade( mu.PI_2 , M.seq )
```

## R code 5.4
And now fit a multivariate regression: divorce ~ Int + ß1Marriage + ß2MedianAgeMarriage
```{r}
m5.3 = map(
  alist(
    Divorce ~ dnorm(mu , sigma) ,
    mu <- a + bR * Marriage.s + bA * MedianAgeMarriage.s ,
    a ~ dnorm(10 , 10) ,
    bR ~ dnorm(0 , 1) ,
    bA ~ dnorm(0 , 1) ,
    sigma ~ dunif(0 , 10)
  ) ,
  data = d
)
precis( m5.3 )

## R code 5.5 - plot the MAP values of each parameter + their percentile intervals
plot( precis(m5.3) )
```

## R code 5.6 - Predictor residual plot
Simply use one predictor to model the other one. Then compute residuals by subtracting real value from the predicted (based on the model). Positive residual means that the marriage rate is above our expectation given the age. Negative means marriage rate below what we'd expect from the State's median marriage age.
```{r}
#model predictor using other predictor
m5.4 = map(
  alist(
    Marriage.s ~ dnorm(mu , sigma) ,
    mu <- a + b * MedianAgeMarriage.s ,
    a ~ dnorm(0 , 10) ,
    b ~ dnorm(0 , 1) ,
    sigma ~ dunif(0 , 10)
  ) ,
  data = d
)

## R code 5.7
# compute expected value at MAP, for each State
mu <- coef(m5.4)['a'] + coef(m5.4)['b']*d$MedianAgeMarriage.s

# compute residual for each State
m.resid <- d$Marriage.s - mu
```

## R code 5.8
And now plot the residual marriage rate in each state after prediction from median marriage age.
```{r}
plot( Marriage.s ~ MedianAgeMarriage.s , d , col=rangi2 )
abline( m5.4 )
# loop over States
for ( i in 1:length(m.resid) ) {
    x <- d$MedianAgeMarriage.s[i] # x location of line segment
    y <- d$Marriage.s[i] # observed endpoint of line segment
    # draw the line segment
    lines( c(x,x) , c(mu[i],y) , lwd=0.5 , col=col.alpha("black",0.7) )
}
```
## R code 5.9
Let's make some counterfactual plots - create some new data, even impossible - easiest by changing one predictor and keeping the others constant. So let's do that with marriage rate while keeping median age constant at mean of the original data.
```{r}
# prepare new counterfactual data
A.avg = mean( d$MedianAgeMarriage.s )
R.seq = seq( from=-3 , to=3 , length.out=30 )
pred.data = data.frame(
    Marriage.s=R.seq,
    MedianAgeMarriage.s=A.avg
)

# compute counterfactual mean divorce (mu)
mu = link( m5.3 , data=pred.data )
mu.mean = apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )

# simulate counterfactual divorce outcomes
R.sim <- sim( m5.3 , data=pred.data , n=1e4 )
R.PI <- apply( R.sim , 2 , PI )

# display predictions, hiding raw data with type="n"
plot( Divorce ~ Marriage.s , data=d , type="n" )
mtext( "MedianAgeMarriage.s = 0" )
lines( R.seq , mu.mean )
shade( mu.PI , R.seq )
shade( R.PI , R.seq )
```

And now keep the marriage rate constant and vary the median age.
```{r}
## R code 5.10
R.avg = mean( d$Marriage.s )
A.seq = seq( from=-3 , to=3.5 , length.out=30 )
pred.data2 = data.frame(
    Marriage.s=R.avg,
    MedianAgeMarriage.s=A.seq
)

mu = link( m5.3 , data=pred.data2 )
mu.mean = apply( mu , 2 , mean )
mu.PI = apply( mu , 2 , PI )

A.sim = sim( m5.3 , data=pred.data2 , n=1e4 )
A.PI = apply( A.sim , 2 , PI )

plot( Divorce ~ MedianAgeMarriage.s , data=d , type="n" )
mtext( "Marriage.s = 0" )
lines( A.seq , mu.mean )
shade( mu.PI , A.seq )
shade( A.PI , A.seq )
```

## R code 5.11
And now compare the implied prediction to training data
```{r}
# call link without specifying new data
# so it uses original data
mu <- link( m5.3 )

# summarize samples across cases
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )

# simulate observations
# again no new data, so uses original data
divorce.sim <- sim( m5.3 , n=1e4 )
divorce.PI <- apply( divorce.sim , 2 , PI )

## R code 5.12
plot( mu.mean ~ d$Divorce , col=rangi2 , ylim=range(mu.PI) ,
    xlab="Observed divorce" , ylab="Predicted divorce" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) )
    lines( rep(d$Divorce[i],2) , c(mu.PI[1,i],mu.PI[2,i]) ,
        col=rangi2 )

## R code 5.13
identify( x=d$Divorce , y=mu.mean , labels=d$Loc , cex=0.8 )
```

## R code 5.14
It's hard to see the prediction error in such plot therefore we use residual plot that show mean prediction error in each row.

```{r}
# compute residuals
divorce.resid = d$Divorce - mu.mean

# get ordering by divorce rate - to show them from lowest prediction error to highest
o = order(divorce.resid)

# make the plot
dotchart( divorce.resid[o] , labels=d$Loc[o] , xlim=c(-6,5) , cex=0.6 )
abline( v=0 , col=col.alpha("black",0.2) )
for ( i in 1:nrow(d) ) {
    j = o[i] # which State in order
    lines( d$Divorce[j]-c(mu.PI[1,j],mu.PI[2,j]) , rep(i,2) )
    points( d$Divorce[j]-c(divorce.PI[1,j],divorce.PI[2,j]) , rep(i,2),
        pch=3 , cex=0.6 , col="gray" )
}
```


## R code 5.15
Overthinking box: how spurious predictor can be correlated to outcome but actually they're both correlated to real causal predictor.
```{r}
N <- 100                         # number of cases
x_real <- rnorm( N )             # x_real as Gaussian with mean 0 and stddev 1
x_spur <- rnorm( N , x_real )    # x_spur as Gaussian with mean=x_real
y <- rnorm( N , x_real )         # y as Gaussian with mean=x_real
d <- data.frame(y,x_real,x_spur) # bind all together in data frame

pairs(d)

#when both x_real and x_spur are entered into regression the real predictor will emerge with posterior mean close to 1 whereas x_spur parameter will be close to 0
```


## R code 5.16
In the "milk" we're interested in hypothesis that animals with larger neocortex have also milk higher in energy (measured in kcal).
```{r}
library(rethinking)
data(milk)
d = milk
str(d)
```

## R code 5.17
Begin with bivariate regression
```{r}
m5.5 = map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bn*neocortex.perc ,
        a ~ dnorm( 0 , 100 ) ,
        bn ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 1 )
    ) ,
    data=d ) #great there are NAs somewhere
```

## R code 5.18
Yep NAs in neocortex % column
```{r}
d$neocortex.perc
sum(is.na(d$neocortex.perc)) #12 NAs


## R code 5.19 - so remove the NAs and work with only complete cases
dcc = d[ complete.cases(d) , ]

## R code 5.20 - and fit the model again using dcc data
m5.5 = map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bn*neocortex.perc ,
        a ~ dnorm( 0 , 100 ) ,
        bn ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 1 )
    ) ,
    data=dcc )
```

## R code 5.21
```{r}
precis( m5.5 , digits=3 ) #bn is very small

## R code 5.22
coef(m5.5)["bn"] * ( 76 - 55 ) #this would be the expected change between the lowest and highest value - the relationship seems small
```

## R code 5.23
Also the 89% interval suggests the estimate is quite unreliable. Plot the predicted mean to see that better.
```{r}
np.seq <- 0:100
pred.data <- data.frame( neocortex.perc=np.seq )

mu <- link( m5.5 , data=pred.data , n=1e4 )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )

plot( kcal.per.g ~ neocortex.perc , data=dcc , col=rangi2 )
lines( np.seq , mu.mean )
lines( np.seq , mu.PI[1,] , lty=2 )
lines( np.seq , mu.PI[2,] , lty=2 )
```

## R code 5.24-5
Let's predict calories of milk from body mass of mother. It's often true that scaled measurements relate by magnitude to other variables. Therefore we log transform the body mass.
```{r}
dcc$log.mass <- log(dcc$mass)

## R code 5.25
m5.6 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bm*log.mass ,
        a ~ dnorm( 0 , 100 ) ,
        bm ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 1 )
    ) ,
    data=dcc )
precis(m5.6)
```
Association is stronger but negative this time and again quite unreliable, both stronger and way weaker effects are possible. Let's plot it again as before.

```{r}
#range of log_mass
range(dcc$log.mass)

mass.seq = seq(-2.5,4.5,length.out = 100)
pred.data2 = data.frame( log.mass=mass.seq )

mu = link( m5.6 , data=pred.data2 , n=1e4 )
mu.mean = apply( mu , 2 , mean )
mu.PI = apply( mu , 2 , PI )

plot( kcal.per.g ~ log.mass , data=dcc , col=rangi2 )
lines( mass.seq , mu.mean )
lines( mass.seq , mu.PI[1,] , lty=2 )
lines( mass.seq , mu.PI[2,] , lty=2 )
```

## R code 5.26
Let's move to the multivariate model now with both neocortex and log.mass as predictors.
```{r}
m5.7 = map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bn*neocortex.perc + bm*log.mass ,
        a ~ dnorm( 0 , 100 ) ,
        bn ~ dnorm( 0 , 1 ) ,
        bm ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 1 )
    ) ,
    data=dcc )
precis(m5.7)
```

## R code 5.27
Let's make a counterfactual plot to show the relationship between energy and neocortex by keeping log.mass constant at its mean.
```{r}
mean.log.mass = mean( log(dcc$mass) )
np.seq = 0:100
pred.data = data.frame(
    neocortex.perc=np.seq,
    log.mass=mean.log.mass
)

mu = link( m5.7 , data=pred.data , n=1e4 )
mu.mean = apply( mu , 2 , mean )
mu.PI = apply( mu , 2 , PI )

plot( kcal.per.g ~ neocortex.perc , data=dcc , type="n" )
lines( np.seq , mu.mean )
lines( np.seq , mu.PI[1,] , lty=2 )
lines( np.seq , mu.PI[2,] , lty=2 )
```

And now the same for log.mass while keeping neocortex.perc constant.
```{r}
mean.neocortex = mean(dcc$neocortex.perc)
mass.seq = seq(-2.5,4.5,length.out = 100)

pred.data <- data.frame(
    log.mass=mass.seq,
    neocortex.perc=mean.neocortex
)

mu <- link( m5.7 , data=pred.data , n=1e4 )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )

plot( kcal.per.g ~ log.mass , data=dcc , type="n" )
lines(mass.seq , mu.mean )
lines(mass.seq , mu.PI[1,] , lty=2 )
lines(mass.seq , mu.PI[2,] , lty=2 )
```
## R code 5.28 
Overthinking box - simulating masking relationship
```{r}
N <- 100                         # number of cases
rho <- 0.5                     # correlation btw x_pos and x_neg
x_pos <- rnorm( N )              # x_pos as Gaussian
x_neg <- rnorm( N , rho*x_pos ,  # x_neg correlated with x_pos
    sqrt(1-rho^2) )
y <- rnorm( N , x_pos - x_neg )  # y equally associated with x_pos, x_neg
d <- data.frame(y,x_pos,x_neg)   # bind all together in data frame
cor(d$x_pos,d$x_neg)
pairs(d)
```

## R code 5.29 - Multicollinearity
First only simulation - predicting height from individuals lenght of both legs.
```{r}
N =100                          # number of individuals
height = rnorm(N,10,2)           # sim total height of each
leg_prop = runif(N,0.4,0.5)      # leg as proportion of height
leg_left= leg_prop*height +     # sim left leg as proportion + error
    rnorm( N , 0 , 0.02 )
leg_right = leg_prop*height +    # sim right leg as proportion + error
    rnorm( N , 0 , 0.02 )
                                  # combine into data frame
d = data.frame(height,leg_left,leg_right)
```

## R code 5.30
Fit model that uses both legs as predictor. We expect association between leg and height around ß~2.2 (because average height=10 /average proportion of legs to rest of height=4.5).
```{r}
m5.8 <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bl*leg_left + br*leg_right ,
        a ~ dnorm( 10 , 100 ) ,
        bl ~ dnorm( 2 , 10 ) ,
        br ~ dnorm( 2 , 10 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis(m5.8)

## R code 5.31
plot(precis(m5.8))
```
So what's happening here?

## R code 5.32 - this should help
Because both predictors include basically the same info they are highly correlated. The model is answering question; what additional info do I get from knowing lenght of right leg after I already saw the left one. The model than basically treats bl and br as the same predictor but associated 2 parameters to them like this; mu= Int + (ß1+ß2)*x. These two never influence mu separately. The resulting posterior is therefore an infinite combination of ß1 and ß2. To get the real association of leg legnth to height we need to sum the posterior of ß1 and ß2 together.
```{r}
post <- extract.samples(m5.8)
plot( bl ~ br , post , col=col.alpha(rangi2,0.1) , pch=16 )
```

## R code 5.33
```{r}
sum_blbr <- post$bl + post$br
dens( sum_blbr , col=rangi2 , lwd=2 , xlab="sum of bl and br" )
```

## R code 5.34
If we fit only one leg then we get approximately the same posterior as the summed one above.
```{r}
m5.9 <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bl*leg_left,
        a ~ dnorm( 10 , 100 ) ,
        bl ~ dnorm( 2 , 10 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis(m5.9)
```

## R code 5.35
Return to the milk data and explore multicollinearity there.
```{r}
library(rethinking)
data(milk)
d <- milk
```

## R code 5.36
First model kcal/g ~ % of fat and % of lactose as separate bivariate regressions.
```{r}
# kcal.per.g regressed on perc.fat
m5.10 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bf*perc.fat ,
        a ~ dnorm( 0.6 , 10 ) ,
        bf ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )

# kcal.per.g regressed on perc.lactose
m5.11 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bl*perc.lactose ,
        a ~ dnorm( 0.6 , 10 ) ,
        bl ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )

precis( m5.10 , digits=3 )
precis( m5.11 , digits=3 )
```

These 2 posteriors are basically opposites of each other.

## R code 5.37
What happens if we use both predictors in the same model.
```{r}
m5.12 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + bf*perc.fat + bl*perc.lactose ,
        a ~ dnorm( 0.6 , 10 ) ,
        bf ~ dnorm( 0 , 1 ) ,
        bl ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis( m5.12 , digits=3 )
```
Both ßs are close to zero now with 89% intervals on both sides of 0. And SDs are twice as large.

## R code 5.38
Easier to see what's happening here.
```{r}
pairs( ~ kcal.per.g + perc.fat + perc.lactose ,
    data=d , col=rangi2 )
```
The scatterplot shows very strong negative correlation between lactose and fat. 

## R code 5.39
Calculate the correlation between them. 
```{r}
cor( d$perc.fat , d$perc.lactose ) #wow baby
```

#Simulation experiment to understand collinearity 
```{r}
library(rethinking)
data(milk)
d = milk
sim.coll = function( r=0.9 ) {
    d$x = rnorm( nrow(d) , mean=r*d$perc.fat ,
        sd=sqrt( (1-r^2)*var(d$perc.fat) ) )
    m = lm( kcal.per.g ~ perc.fat + x , data=d )
    sqrt( diag( vcov(m) ) )[2] # stddev of parameter
}
rep.sim.coll = function( r=0.9 , n=100 ) {
    stddev = replicate( n , sim.coll(r) )
    mean(stddev)
}
r.seq = seq(from=0,to=0.99,by=0.001)
stddev = sapply( r.seq , function(z) rep.sim.coll(r=z,n=100) )
plot( stddev ~ r.seq , type="l" , col=rangi2, lwd=2 , xlab="correlation" )
```



## R code 5.41 Post-treatment bias
Worrying about mistaken inferences arising from including variables that are consequences of other.
An example: interested in effect of treatment of heigth of plants. +we presence of fungus after treatment is measured (only 1 or 0). It's wrong to include fungus variable because that's post-treatment. Let's see why and what happens if we do.
```{r}
# number of plants
N <- 1e3

# simulate initial heights
h0 <- rnorm(N,10,2)

# assign treatments and simulate fungus and growth
treatment <- rep( 0:1 , each=N/2 )
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 )
h1 <- h0 + rnorm(N, 5 - 3*fungus)

# compose a clean data frame
d <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )
```

## R code 5.42
```{r}
m5.13 <- map(
    alist(
        h1 ~ dnorm(mu,sigma),
        mu <- a + bh*h0 + bt*treatment + bf*fungus,
        a ~ dnorm(0,100),
        c(bh,bt,bf) ~ dnorm(0,10),
        sigma ~ dunif(0,10)
    ),
    data=d )
precis(m5.13)
```
So the effect of treatment is very small now and effect of fungus is quite big although the simulation data is constrructed so that treatment really does matter. It's mainly because fungus is consequence of treatment.

## R code 5.43
Since we're interested in effect of soil treatment we should omit the fungus predictor
```{r}
m5.14 <- map(
    alist(
        h1 ~ dnorm(mu,sigma),
        mu <- a + bh*h0 + bt*treatment,
        a ~ dnorm(0,100),
        c(bh,bt) ~ dnorm(0,10),
        sigma ~ dunif(0,10)
    ),
    data=d )
precis(m5.14)
```

## R code 5.44 - Categorical variables
```{r}
data(Howell1)
d <- Howell1
str(d)
```

## R code 5.45
Fit model: height ~ male
```{r}
m5.15 <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bm*male ,
        a ~ dnorm( 178 , 100 ) ,
        bm ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
    ) ,
    data=d )
precis(m5.15,corr = T)
```

## R code 5.46
Adding a+bm results in posterior mean average male height. To consider also the width of posterior we can't just sum the boundaries in precis because they're correlated. But sampling fixes the problam.
```{r}
post <- extract.samples(m5.15)
mu.male <- post$a + post$bm
PI(mu.male)
```

## R code 5.47 Overthinking: Re-parameterizing
This is the same model as above just with creating a parameter for both genders
```{r}
m5.15b <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- af*(1-male) + am*male ,
        af ~ dnorm( 178 , 100 ) ,
        am ~ dnorm( 178 , 100 ) ,
        sigma ~ dunif( 0 , 50 )
    ) ,
    data=d )

precis(m5.15b)
```

## R code 5.48 Many categories
We need to create one less dummy variables than categories. The last category will be intercept.
```{r}
data(milk)
d <- milk
unique(d$clade)

## R code 5.49
( d$clade.NWM <- ifelse( d$clade=="New World Monkey" , 1 , 0 ) )

## R code 5.50
d$clade.OWM <- ifelse( d$clade=="Old World Monkey" , 1 , 0 )
d$clade.S <- ifelse( d$clade=="Strepsirrhine" , 1 , 0 )
```

## R code 5.51
```{r}
m5.16 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a + b.NWM*clade.NWM + b.OWM*clade.OWM + b.S*clade.S ,
        a ~ dnorm( 0.6 , 10 ) ,
        b.NWM ~ dnorm( 0 , 1 ) ,
        b.OWM ~ dnorm( 0 , 1 ) ,
        b.S ~ dnorm( 0 , 1 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis(m5.16)
```

## R code 5.52
Now we want the posterior distributions of average milk energy. We use samples.
```{r}
# sample posterior
post <- extract.samples(m5.16)

# compute averages for each category
mu.ape <- post$a
mu.NWM <- post$a + post$b.NWM
mu.OWM <- post$a + post$b.OWM
mu.S <- post$a + post$b.S

# summarize using precis
precis( data.frame(mu.ape,mu.NWM,mu.OWM,mu.S) )
```

## R code 5.53
To re-parameterize model to estimate the difference between two categories we just subtract the two estimates +get 95% interval + median
```{r}
diff.NWM.OWM <- mu.NWM - mu.OWM
quantile( diff.NWM.OWM , probs=c(0.025,0.5,0.975) )

precis(data.frame(diff.NWM.OWM))
```

## R code 5.54
Another approach is to specify unique intercept for each category.
```{r}
d$clade_id = coerce_index(d$clade)

## R code 5.55
m5.16_alt = map(
    alist(
        kcal.per.g ~ dnorm( mu , sigma ) ,
        mu <- a[clade_id] ,
        a[clade_id] ~ dnorm( 0.6 , 10 ) ,
        sigma ~ dunif( 0 , 10 )
    ) ,
    data=d )
precis( m5.16_alt , depth=2 )
```


## R code 5.56 Ordinary least squares
```{r}
m5.17 <- lm( y ~ 1 + x , data=d )
m5.18 <- lm( y ~ 1 + x + z + w , data=d )

## R code 5.57
m5.17 <- lm( y ~ 1 + x , data=d )
m5.19 <- lm( y ~ x , data=d )

## R code 5.58
m5.20 <- lm( y ~ 0 + x , data=d )
m5.21 <- lm( y ~ x - 1 , data=d )

## R code 5.59
m5.22 <- lm( y ~ 1 + as.factor(season) , data=d )

## R code 5.60
d$x2 <- d$x^2
d$x3 <- d$x^3
m5.23 <- lm( y ~ 1 + x + x2 + x3 , data=d )

## R code 5.61
m5.24 <- lm( y ~ 1 + x + I(x^2) + I(x^3) , data=d )

## R code 5.62  - glimmer function can turn lm formula into map formula
d=d[complete.cases(d),]
glimmer(kcal.per.g ~ neocortex.perc + (1|clade) , data=d )
```