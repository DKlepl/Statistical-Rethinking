---
title: "Chapter 3"
author: "Dominik Klepl"
date: "17 2 2018"
output: html_document
---

```{r setup}
library(rethinking)
```

## R code 3.1
```{r}
#test for vampirism that diagnoses vampire 95% of time
Pr_pos_vam = 0.95

#there are false positives - human diagnozed as vampire
Pr_pos_hum = 0.01

#vampires are very rare in population
Pr_vam = 0.001

#calculate Pr of test being positive
Pr_pos = Pr_pos_vam*Pr_vam + Pr_pos_hum*(1-Pr_vam)

#what is the probability of the subject being vampire given a positive test
Pr_vam_pos = Pr_pos_vam*Pr_vam / Pr_pos

print(Pr_vam_pos) #=> the probability is 8.7%
```

## R code 3.2-5 (p.52)
```{r}
#working with posterior distribution of globe tossing example in chapter 2

#compute posterior by using grid approximation (same as in Ch.2)
p_grid = seq( from=0 , to=1 , length.out=1000 )
prior = rep( 1 , 1000 )
likelihood = dbinom( 6 , size=9 , prob=p_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)

#draw randomly 10000 samples from the grid of parameters whose proportion in the grid is given by posterior
sample_size=10^4
samples = sample( p_grid , prob=posterior , size=sample_size , replace=TRUE )

#plot these samples
plot (samples) # a lot around 0.6

#show the estimated density of probabilities
dens (samples) #this is code from the book

#would be maybe nicer with ggplot
library(ggplot2)

ggplot(as.data.frame(samples),aes(x=samples))+
  geom_histogram(stat="density")

#looks very similar to the grid approximation - more samples=smoother curve
```

## R code 3.6-7 (p.54)
```{r}
#summarizing posterior prob. 
# What is the probability of proportion of water being under 0.5

#first from directly from grid approximation
sum( posterior[ p_grid < 0.5 ] )

#often not possible (when there are more parameters) +grid isn't practical
#count the probabilities below 0.5 in the samples and divide by full samples to get a frequency
sum( samples < 0.5 ) / sample_size

#similarly a % of probs lying between 0.5 and 0.75
sum( samples > 0.5 & samples < 0.75 ) / sample_size
```

## R code 3.9 (p.55-6)
Interval of defined mass = confidence intervals or credible intervals

Get boundaries of lower 80% of the posterior - lower is of course 0 and the end is simply 80th percentile.
```{r}
quantile( samples , 0.8 )
```

Get middle 80% of posterior - so between 10th and 90th percentile.
```{r}
quantile( samples , c( 0.1 , 0.9 ) )
```
These are "percentile intervals" (PI) because they assign the same mass to both tails.

## R code 3.11-13 (p.56)
PIs are good with symmetrical distributions but not so much with asymetrical. Considering highly skewed distribution (getting 3x water in 3 tosses). 
```{r}
#do grid approximation
p_grid = seq( from=0 , to=1 , length.out=1000 )
prior = rep(1,1000)
likelihood = dbinom( 3 , size=3 , prob=p_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)

#draw samples from posterior
sample_size=10^4
samples = sample( p_grid , size=sample_size , replace=TRUE , prob=posterior )

dens(samples)
```

Let's look at the difference in PI and "highest posterior density interval" (HPDI). The highest probability density is around 1 but the PI leaves it out because it assigns the same mass (25%) to both sides and provides central probability. But there is basically infinite number of such PIs (25%-75% but also 20%-70% etc.). HPDI shows 50% interval around the probability with highest density.
```{r}
PI( samples , prob=0.5 )

## R code 3.13
HPDI( samples , prob=0.5 )
```


## R code 3.14-16
Point estimates. First 2 codes compute MAP (maximum a posteriori) - from grid and from samples
```{r}
#from grid
p_grid[ which.max(posterior) ]

#from samples
chainmode( samples , adj=0.01 )

#why not mean or median instead?
mean( samples )
median( samples )

```
## R code 3.17-19 (p.60)
Calculating loss function (d-p) and looking for point in the distribution where the loss is minimal.
```{r}
#if the decision (point to report as closest to true value, for globe tossing it's the true proportion of water) is 0.5 than we average over the uncertainty in the true value - information about the parameter in the model is the whole posterior 
sum( posterior*abs( 0.5 - p_grid ) )

##repeat this for every possible value of d (=decision, 0.5 above)
loss = sapply( p_grid , function(d) sum( posterior*abs( d - p_grid ) ) )

#plot loss function
ggplot(as.data.frame(loss),aes(x=p_grid,y=loss))+
  geom_line()

#minimal loss => median
p_grid[ which.min(loss) ]
median(samples)
```

Repeat with loss function (d-p)^2
```{r}
loss_quad = sapply( p_grid , function(d) (sum( posterior*abs( d - p_grid )^2) ) )

ggplot(as.data.frame(loss_quad),aes(x=p_grid,y=loss_quad))+
  geom_line()

p_grid[ which.min(loss_quad) ]
mean(samples)
```

## R code 3.20
Likelihood function can be reversed and instead of producing probability of observed data generate dummy data (observations given the probability).

First compute probability of every option with 2 tosses given the proportion of 70% water. These are 0,1 or 2 waters.
```{r}
dbinom( 0:2 , size=2 , prob=0.7 )
```


## R code 3.21
Now from the distribution above draw random samples => generate dummy data (0,1 or 2)
```{r}
#generate one observations
rbinom( 1 , size=2 , prob=0.7 )

#or 10
rbinom( 10 , size=2 , prob=0.7 )

#or 10 thousand
dummy_w = rbinom( 1e5 , size=2 , prob=0.7 )

#and calculate the proportion of observations - cca same as the likelihoods above
table(dummy_w)/1e5
```

## R code 3.24
```{r}
#now use 9 tosses and same proportion of water
dummy_w = rbinom( 10^5 , size=100, prob=0.7 )

#and make histogram of dummy data
simplehist( dummy_w , xlab="dummy water count" )
```



## R code 3.25-26
To get posterior predictive distribution dummy data are drawn from distributions of all possible values of parameter p (proportion of water) and weighted with its respective posterior probability
```{r}
#example of one dummy data sampling
w = rbinom( 1e4 , size=9 , prob=0.6 )

#get the samples again from the right posterior
p_grid = seq( from=0 , to=1 , length.out=1000 )
prior = rep( 1 , 1000 )
likelihood = dbinom( 6 , size=9 , prob=p_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)

#draw randomly 10000 samples from the grid of parameters whose proportion in the grid is given by posterior
sample_size=10^4
samples = sample( p_grid , prob=posterior , size=sample_size , replace=TRUE )

w = rbinom( 1e5 , size=9 , prob=samples )

simplehist(w)
```

#Practice

##EASY
```{r}
#data to use
p_grid = seq( from=0 , to=1 , length.out=1000 )
prior = rep( 1 , 1000 )
likelihood = dbinom( 6 , size=9 , prob=p_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)
set.seed(100) #to make the samples stable and comparable to results in the book
samples = sample( p_grid , prob=posterior , size=10^4 , replace=TRUE )
```

###3E1
How much post. prob. lies below 0.2?
```{r}
sum( samples < 0.2 ) / 10^4
sum(posterior[p_grid<0.2])
```
###3E2
How much post. prob. lies above 0.8?
```{r}
sum( samples > 0.8 ) / 10^4
sum(posterior[p_grid>0.8])
```

###3E3
How much post. prob. lies between 0.2 and 0.8?
```{r}
sum( samples > 0.2 & samples < 0.8 ) / 10^4

#and answers from 1, 2 and 3 adds up to 1
```

###3E4 and 3E5

```{r}
quantile( samples , 0.2 )

quantile( samples ,  c(0.8,1))
```

###3E6
```{r}
HPDI(samples, prob=.66)
```

###3E7
```{r}
PI(samples,prob=.66)
```


#MEDIUM

###3M1
```{r}
p_grid = seq( from=0 , to=1 , length.out=1000 )
prior = rep( 1 , 1000 )
likelihood = dbinom( 8 , size=15 , prob=p_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)
set.seed(100) #to make the samples stable
```

###3M2
```{r}
samples = sample( p_grid , prob=posterior , size=10^4 , replace=TRUE )
HPDI(samples,prob=.90)
```

###3M3
```{r}
post_pred_distr = sample( p_grid , prob=posterior , size=10^4 , replace=TRUE )

samples = rbinom( 10^4 , size=15 , prob=post_pred_distr )
mean(samples==8)
```

###3M4
```{r}
samples = rbinom( 10^4 , size=9 , prob= post_pred_distr)
mean(samples==6)
```

###3M5
```{r}
p_grid = seq( from=0 , to=1 , length.out=1000 )
prior = ifelse(test = p_grid < .5, yes = 0, no = 1)
likelihood = dbinom( 8 , size=15 , prob=p_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)
set.seed(100) #to make the samples stable and comparable to results in the book

#repeat 3M2
samples = sample( p_grid , prob=posterior , size=10^4 , replace=TRUE )
HPDI(samples,prob=.90)

#repeat 3M3
post_pred_distr = sample( p_grid , prob=posterior , size=10^4 , replace=TRUE )

samples = rbinom( 10^4 , size=15 , prob=post_pred_distr )
mean(samples==8)

samples = rbinom( 10^4 , size=9 , prob= post_pred_distr)
mean(samples==6)
```

## HARD
### Intro
```{r}
## R code 3.29
library(rethinking)
data(homeworkch3)

## R code 3.30
data = as.data.frame(cbind(birth1,birth2))
```

### 3H1
```{r}
all_births = nrow(data)*2
boys = sum(data)
girls = all_births-boys

#get posterior distribution with grid approximation
p_grid = seq( from=0 , to=1 , length.out=1000 )
prior = rep( 1 , 1000 )
likelihood = dbinom( boys , size=all_births , prob=p_grid )
posterior = likelihood * prior
posterior = posterior / sum(posterior)
set.seed(100) #to make the samples stable

plot(posterior ~ p_grid, type = "l")

p_grid[which.max(posterior)]
```

###3H2
```{r}
samples = sample(p_grid, prob=posterior,size=10^4,replace=T)

intervals = c(.5,.89,.97)

for (i in intervals) {
  print(HPDI(samples,i))
}
```

### 3H3
```{r}
predictions = rbinom( 10^4 , size=200 , prob=samples )

dens(predictions)

dens(predictions)
  abline(v = boys, col = "red") #model seems to be fitting pretty nicely
```

### 3H4
```{r}
#get samples again
samples = sample(p_grid, prob=posterior,size=10^4,replace=T)

#count number of boys born as the first-child
first_boy = sum(data[,1])
births=100

simul = rbinom( 10^4 , size=100 , prob=samples )

dens(simul,adj=1)
  abline(v=first_boy, col="red") #that's not very good, although still kinda close
```

### 3H5
```{r}
boys_after_girls=sum(data$birth2[data$birth1==0])
first_girl = as.numeric(sum(data$birth1==0))

sims = rbinom(10^4, size=first_girl,prob=samples)

dens(sims) 
  abline(v=boys_after_girls, col="red") #births are not independent therefore model performs badly
```

